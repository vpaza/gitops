apiVersion: logging.banzaicloud.io/v1beta1
kind: Logging
metadata:
  name: logging
  namespace: logging
spec:
  fluentd: {}
  fluentbit: {}
  controlNamespace: logging
  watchNamespaces: ['zdv','ingress-nginx']
---
apiVersion: logging.banzaicloud.io/v1beta1
kind: ClusterOutput
metadata:
  name: s3-output
  namespace: logging
spec:
  s3:
    aws_key_id:
      valueFrom:
        secretKeyRef:
          name: logging-s3
          key: awsAccessKeyId
    aws_sec_key:
      valueFrom:
        secretKeyRef:
          name: logging-s3
          key: awsSecretAccessKey
    s3_bucket: kzdv-backups
    s3_endpoint: https://sfo3.digitaloceanspaces.com
    s3_region: us-east-1
    path: logs/%Y-%m-%d/${tag}/
    buffer:
      timekey: 60m
      timekey_wait: 30s
      timekey_use_utc: true
---
apiVersion: logging.banzaicloud.io/v1beta1
kind: ClusterFlow
metadata:
  name: s3-flow
  namespace: logging
spec:
  filters:
    - tag_normaliser:
        format: ${namespace_name}.${pod_name}.${container_name}
  globalOutputRefs:
    - s3-output
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: log-cleanup
  namespace: logging
spec:
  concurrencyPolicy: Allow
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - command:
            - /bin/bash
            - -c
            - |
              set -e
              mkdir ~/.aws
              cat <<EOF >~/.aws/config
              [default]
              output = json
              region = us-east-1
              EOF
              cat <<EOF >~/.aws/credentials
              [default]
              aws_access_key_id = ${AWS_ACCESS_KEY}
              aws_secret_access_key = ${AWS_SECRET_KEY}
              EOF
              set -x
              aws s3 --endpoint=https://${AWS_ENDPOINT} ls s3://${AWS_BUCKET}/logs/ | while read -r line; do
                  createDate=`echo $line|awk {'print $2'}|tr -d "/"`
                  createDate=`date -d"$createDate" +%s`
                  olderThan=`date --date "90 days ago" +%s`
                  if [[ $createDate -lt $olderThan ]]; then
                    filename=`echo $line|awk {'print $2'}`
                    aws s3 --endpoint=https://${AWS_ENDPOINT} rm --recursive s3://${AWS_BUCKET}/logs/$filename
                  fi
              done
            env:
            - name: AWS_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: logging-s3
                  key: awsAccessKeyId
            - name: AWS_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: logging-s3
                  key: awsSecretAccessKey
            - name: AWS_ENDPOINT
              value: sfo3.digitaloceanspaces.com
            - name: AWS_BUCKET
              value: kzdv-backups
            image: denartcc/cli:v1.1.2-1
            imagePullPolicy: Always
            name: backup
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: OnFailure
          schedulerName: default-scheduler
          terminationGracePeriodSeconds: 30
  schedule: 0 0 * * *
  successfulJobsHistoryLimit: 3
  suspend: false
